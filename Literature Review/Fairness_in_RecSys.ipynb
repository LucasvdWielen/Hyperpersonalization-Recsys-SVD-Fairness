{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef50dcf",
   "metadata": {},
   "source": [
    "# Fairness in Recommender Systems - Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7e95b",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69436b48",
   "metadata": {},
   "source": [
    "Artificial intelligence (AI) and machine learning (ML) systems are increasingly integrated into decision-making processes across domains like finance, healthcare, hiring, and digital platforms. Despite enhancing efficiency and personalization, these systems raise ethical concerns regarding algorithmic bias and fairness (Deldjoo et al., 2024; Mukherjee et al., 2020). Fairness is crucial to prevent systemic discrimination and inequitable treatment of users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b5d53",
   "metadata": {},
   "source": [
    "Recommender systems (RSs) are widely used AI applications shaping user experiences in e-commerce, social media, streaming, and job recommendations. However, they are often optimized for engagement and revenue at the cost of fair content exposure and equitable user treatment (Raza, 2024), raising concerns about biased personalization and reinforcing societal inequalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19898d3a",
   "metadata": {},
   "source": [
    "Fairness-aware machine learning identifies two fairness perspectives (Dwork et al., 2012; Mitchell et al., 2020):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bab2fe",
   "metadata": {},
   "source": [
    "Process fairness: Preventing bias during model training through fairness constraints (Yao & Huang, 2017; Ekstrand et al., 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af6c64",
   "metadata": {},
   "source": [
    "Outcome fairness: Ensuring recommendations do not disproportionately harm specific individuals or groups (Burke, 2017; Jin et al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a7d84",
   "metadata": {},
   "source": [
    "This literature review focuses on outcome fairness, as it directly impacts users and content providers. Process fairness does not always ensure fair outcomes (Li et al., 2023), especially when RSs operate as black boxes where post-hoc evaluation and correction of outcomes are more feasible (Rampisela et al., 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368747eb",
   "metadata": {},
   "source": [
    "# Sources of (un)fairness in recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b3dfb",
   "metadata": {},
   "source": [
    "Recommender systems play a central role in shaping user experiences across digital platforms, but their design also introduces several sources of unfairness. These sources are interconnected, with fairness conflicts driving algorithmic biases, which are further reinforced through data dependencies and long-term system dynamics. Understanding these sources is essential to address the root causes of unfairness in RSs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b81e4",
   "metadata": {},
   "source": [
    "## 2.1 Conflicting fairness definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c5f4d",
   "metadata": {},
   "source": [
    "Fairness definitions in RSs vary among stakeholders. Users seek relevant recommendations, content providers desire fair exposure, and platforms prioritize engagement and revenue (Ekstrand et al., 2022). Balancing these competing objectives often leads to trade-offs where optimizing for one perspective may reduce fairness for others. For instance, increasing exposure for niche providers can diminish recommendation relevance for users. These conflicts become embedded in system design, resulting in persistent fairness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e270f",
   "metadata": {},
   "source": [
    "## 2.2 Algorithmic bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e033d5",
   "metadata": {},
   "source": [
    "Algorithmic bias is the most direct manifestation of these fairness conflicts. It occurs when the system’s design and optimization choices systematically favour certain users, content types, or behaviours over others. Rather than random errors, these biases are often hard coded into how relevance is measured, how ranking is performed, or what objectives are prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f356d",
   "metadata": {},
   "source": [
    "Several types of algorithmic bias are common in RSs. For example, ranking bias ensures that certain content consistently appears at the top of recommendation lists, while position bias amplifies this effect by increasing the likelihood that users click on highly ranked items, regardless of their actual relevance (Singh & Joachims, 2018; Chen et al., 2023). Similarly, optimizing for engagement metrics like clicks or watch time can introduce objective function bias, where the system rewards content that maximizes these metrics but may sideline fairness goals (Zhu et al., 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90783c",
   "metadata": {},
   "source": [
    "Further, feature representation bias occurs when certain attributes of users or items are underrepresented or poorly encoded, leading to systematic disadvantages (Geyik et al., 2019). Relatedly, diversity and conformity biases arise when the system promotes homogeneous content, reinforcing mainstream preferences and reducing exposure to minority or niche content (Ekstrand et al., 2022; Chen et al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c200f",
   "metadata": {},
   "source": [
    "What unifies these examples is that they all result from deliberate design decisions made during model development and training. Because these biases originate from conflicting fairness goals embedded within the system, they are difficult to detect and correct without fundamentally reconsidering what the system is optimizing for. For a full overview of specific algorithmic biases found in recent literature, see Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd142f5",
   "metadata": {},
   "source": [
    "## 2.3 Data bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee06c7",
   "metadata": {},
   "source": [
    "While algorithms shape fairness through optimization objectives, data bias presents a parallel challenge that can be just as impactful. Even a perfectly designed algorithm cannot operate fairly if the data it learns from is skewed, incomplete, or historically biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4c596",
   "metadata": {},
   "source": [
    "Data bias enters the system through several mechanisms. Demographic bias, for example, occurs when certain groups are over- or underrepresented in the dataset, causing the system to perform better for some users than others (Chen et al., 2020). Sampling bias can arise when data collection methods exclude certain behaviours or user groups, making the model blind to their preferences (Lahoti et al., 2019). Additionally, popularity bias leads the model to overemphasize historically popular content, crowding out lesser-known or niche items (Mehrotra et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb24d6",
   "metadata": {},
   "source": [
    "These biases do not just influence initial recommendations but become entrenched over time as the system continues to learn from new data that reflects its past biases. This creates a cycle where data and algorithmic design interact to reinforce unfair patterns. A detailed overview of common data biases is also included in Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5c1b2",
   "metadata": {},
   "source": [
    "Table 1 Bias metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554167a",
   "metadata": {},
   "source": [
    "## 2.4 Feedback loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593069f",
   "metadata": {},
   "source": [
    "Perhaps the most concerning aspect of bias in RSs is how it evolves and compounds through feedback loops. Once unfair patterns are established whether through algorithmic or data bias, they tend to reinforce themselves. Biased recommendations influence user behaviour, generating interaction data that reflects these biases. In turn, this data is used to train the next generation of models, deepening the unfairness over time (Ekstrand et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3838e91",
   "metadata": {},
   "source": [
    "For instance, when popular items are consistently recommended and clicked, self-reinforcing disparities emerge, making it even harder for less popular content to surface (Singh & Joachims, 2018). Similarly, user behaviour amplification occurs when repetitive exposure to the same types of content shapes user preferences, narrowing their consumption patterns (Mehrotra et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779eef5",
   "metadata": {},
   "source": [
    "These feedback loops demonstrate that fairness in RSs is not a static problem but a dynamic process where initial design choices and data limitations can escalate into systemic unfairness if left unaddressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796302e7",
   "metadata": {},
   "source": [
    "## 2.5 Cold Start Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796bf0e",
   "metadata": {},
   "source": [
    "The cold start problem adds another layer of complexity to fairness challenges in RSs. When new users or new items enter the system, there is often insufficient data to generate personalized recommendations. As a result, the system tends to fall back on recommending popular or generic content, further entrenching popularity bias and limiting opportunities for new items or users to gain visibility (Li et al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1ac66",
   "metadata": {},
   "source": [
    "This phenomenon disproportionately affects minority users or niche content, making it difficult for them to compete on equal footing with established players. It also delays the system’s ability to learn about diverse preferences, worsening the overall fairness of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c6c5b",
   "metadata": {},
   "source": [
    "Figure 1 Part of the conceptual model about sources of (un)fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5b699",
   "metadata": {},
   "source": [
    "# Conceptualizing fairness in recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99060d",
   "metadata": {},
   "source": [
    "After understanding where unfairness originates, it is essential to establish a framework for what fairness means in the context of recommender systems. Given their role in shaping exposure, opportunities, and user experiences, fairness in RSs is both multi-dimensional and highly contextual. Fairness involves decisions about who deserves access to recommendations, how these are distributed, and whether the outcomes reinforce or reduce existing inequalities. This section conceptualizes fairness along key dimensions, as visualized in Figure 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f3daf",
   "metadata": {},
   "source": [
    "## 3.1 Stakeholder perspectives on fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf1900",
   "metadata": {},
   "source": [
    "Fairness in recommender systems (RSs) is inherently relational, varying by stakeholder expectations. Users seek recommendations that match their preferences, provide diversity, and avoid discrimination, while content providers aim for visibility in competitive environments. Platforms prioritize engagement, profitability, and regulatory compliance (Deldjoo et al., 2024; Rampisela et al., 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d031295",
   "metadata": {},
   "source": [
    "Literature identifies several stakeholder-specific fairness dimensions. User fairness focuses on providing relevant, diverse recommendations regardless of demographics or behavior (Wang et al., 2023). Item fairness ensures content is not systematically underexposed (Rampisela et al., 2024). Provider fairness emphasizes the visibility of creators competing for user attention (Singh & Joachims, 2018). Platform fairness involves balancing stakeholder demands while achieving organizational objectives (Chaudhari et al., 2020). Multi-stakeholder fairness acknowledges that these interests often conflict, requiring continuous negotiation (Burke, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b3ea0",
   "metadata": {},
   "source": [
    "## 3.2 Fairness over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5703c2f",
   "metadata": {},
   "source": [
    "Fairness in recommender systems (RSs) is dynamic, evolving through repeated user interactions that continuously influence system data and potentially reinforce biases. Temporal fairness, often overlooked, addresses this evolving nature (Ekstrand et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2b5f4",
   "metadata": {},
   "source": [
    "Biega et al. (2018) differentiate between short-term fairness, which addresses immediate recommendation outcomes, and long-term fairness, which considers cumulative exposure imbalances that can systematically underrepresent certain items or groups. Li et al. (2023) emphasize that effective fairness interventions must address these long-term dynamics to prevent reinforcement of popularity biases. Ensuring fairness in RSs requires ongoing attention to how systems learn and adapt over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36194e1",
   "metadata": {},
   "source": [
    "## 3.3 Process and outcome fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef046d6a",
   "metadata": {},
   "source": [
    "Fairness in recommender systems (RSs) involves process fairness and outcome fairness, two concepts from fairness-aware machine learning (Dwork et al., 2012; Mitchell et al., 2020). Process fairness ensures algorithms treat similar individuals similarly during training and prediction, achieved through fairness constraints, balanced data, or debiasing techniques (Ekstrand et al., 2022; Yao & Huang, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c873f",
   "metadata": {},
   "source": [
    "However, RSs often operate as black boxes, making process observation and intervention difficult. As a result, fairness assessments typically focus on outcomes, evaluating whether recommendations lead to unequal or discriminatory results for specific users or groups (Burke, 2017; Jin et al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5432bd",
   "metadata": {},
   "source": [
    "This review emphasizes outcome fairness, as it directly affects users and content providers, regardless of the system’s internal workings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf8d66",
   "metadata": {},
   "source": [
    "## 3.4 Operationalizing outcome fairness through distributive fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ba5b6",
   "metadata": {},
   "source": [
    "Outcome fairness in recommender systems (RSs) is often addressed through distributive fairness, which focuses on how limited resources such as recommendation slots and user attention are allocated (Singh et al., 2021). Rooted in economic theories of fairness, distributive fairness assesses equity by examining how benefits and burdens are shared among stakeholders (Chouldechova & Roth, 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3834a",
   "metadata": {},
   "source": [
    "Distributive fairness serves as a foundational layer for evaluating fairness outcomes. It ensures that individual and group fairness metrics are applied to a balanced distribution of exposure and relevance. This approach is necessary because evaluating fairness outcomes requires first assessing whether the underlying distribution of resources is balanced. When distributions are skewed, outcome fairness cannot be accurately measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cce9f",
   "metadata": {},
   "source": [
    "In the context of RSs, distributive fairness aims to prevent the excessive concentration of exposure on a limited set of users or items. Biega et al. (2018) emphasize the importance of exposure fairness, ensuring that minority content providers have fair opportunities to reach audiences. Without explicit fairness constraints, RSs tend to favor popular content, reinforcing inequalities over time (Diaz et al., 2020). Burke (2017) and Singh & Joachims (2018) demonstrate that distributive fairness metrics provide effective tools for auditing and enhancing fairness in RS outputs, particularly in large-scale platforms with intense competition for visibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770ef11",
   "metadata": {},
   "source": [
    "## 3.5 Individual and group fairness within distributive fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cc674",
   "metadata": {},
   "source": [
    "Distributive fairness in recommender systems (RSs) distinguishes between individual and group fairness. Individual fairness ensures similar users receive comparable recommendations, maintaining relevance and diversity based on their preferences or behavior histories (Dwork et al., 2012). Lahoti et al. (2019) enhance this concept through pairwise fairness measures that capture subtle individual-level disparities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e33ba",
   "metadata": {},
   "source": [
    "Group fairness addresses aggregate outcomes across defined groups, aiming to prevent disparities in exposure. Methods include disparate impact ratios to measure exposure imbalances (Zafar et al., 2017) and demographic parity, which ensures recommendation probabilities are evenly distributed across groups (Feldman et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7cead",
   "metadata": {},
   "source": [
    "Chen et al. (2023) emphasize the need to balance individual and group fairness, noting that prioritizing group fairness can still result in unfair treatment of individuals. Burke (2017) highlights the complexity of designing fair systems that navigate these trade-offs effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76881dd",
   "metadata": {},
   "source": [
    "Figure 2 Part of the conceptual model about concepts of fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06effdf0",
   "metadata": {},
   "source": [
    "# Fairness metrics in recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5b54b",
   "metadata": {},
   "source": [
    "Fairness in recommender systems requires translating theoretical principles into measurable metrics to assess disparities and the impact of fairness interventions. As fairness is context-dependent, selecting appropriate metrics is crucial. This review follows Wang et al. (2023) in categorizing fairness metrics into six key dimensions while integrating insights from other studies on exposure fairness, demographic parity, and individual fairness. While Wang et al. (2023) provides this structure, the specific fairness metrics discussed in this review are drawn from a broader set of studies to ensure a more comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17224cf3",
   "metadata": {},
   "source": [
    "Why Fairness Metrics Matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a4ffa",
   "metadata": {},
   "source": [
    "Fairness metrics are critical because algorithmic biases in recommender systems can perpetuate societal inequalities, affecting job opportunities, financial access, and content visibility. Accurate metrics guide interventions to balance personalization, equity, and stakeholder interests. Systems optimized solely for engagement may disproportionately benefit certain groups, reinforcing existing disparities. Table 2 presents key fairness metrics categorized into individual, group, and bridging measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e042f5",
   "metadata": {},
   "source": [
    "Table 2 Fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec44c00",
   "metadata": {},
   "source": [
    "## 4.1 Individual Fairness Metrics in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33c232",
   "metadata": {},
   "source": [
    "Ensuring fairness at the individual level means users with similar preferences and characteristics should receive comparable recommendations. Achieving this in real-world recommender systems is challenging due to complex user interactions and inherent biases in training data. Several metrics help assess individual fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436cf84",
   "metadata": {},
   "source": [
    "Local Individual Fairness evaluates whether users with similar behaviors receive comparable recommendations. For example, two shoppers with nearly identical purchase histories should not receive drastically different product suggestions, as this may signal inconsistencies or biases in the recommendation model (Yao & Huang, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8258b9b",
   "metadata": {},
   "source": [
    "Consistency Score measures whether small changes in user behavior lead to disproportionate shifts in recommendations. In personalized news feeds, reading a single article should not drastically alter future suggestions, as such abrupt changes can reinforce filter bubbles and limit exposure to diverse viewpoints (Dwork et al., 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b60394",
   "metadata": {},
   "source": [
    "Fairness Graph Distance analyzes relationships between users, ensuring that those with similar attributes receive proportionally similar recommendations. For instance, fairness audits on platforms like Spotify or Netflix can identify whether demographic groups with comparable histories receive systematically different recommendations, revealing biases in collaborative filtering models (Lahoti et al., 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc476ad4",
   "metadata": {},
   "source": [
    "Counterfactual Difference assesses how recommendations change when sensitive attributes, like gender or ethnicity, are altered. If a job recommendation platform offers different roles or salaries based on gender, counterfactual fairness is violated, potentially reinforcing workplace inequalities (Kusner et al., 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3bd27",
   "metadata": {},
   "source": [
    "These individual fairness metrics are crucial but may conflict with group fairness goals, where broader systemic disparities require adjustments. Thus, achieving fairness in RSs requires balancing personalized recommendations with addressing biases at both individual and group levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64440df9",
   "metadata": {},
   "source": [
    "Figure 3 Individual fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efaf50",
   "metadata": {},
   "source": [
    "## 4.2 Group Fairness Metrics in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b2135",
   "metadata": {},
   "source": [
    "While individual fairness ensures users receive personalized recommendations without arbitrary disparities, group fairness focuses on equity across demographic or content groups. Recommender systems often reinforce pre-existing societal biases, necessitating metrics to assess equitable distribution across groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f994e1",
   "metadata": {},
   "source": [
    "Exposure bias is a major concern in group fairness, where popular content dominates recommendations, limiting visibility for niche creators. This is evident on platforms like YouTube and TikTok, where trending content overshadows minority creators. Exposure fairness metrics help ensure that underrepresented creators still gain visibility, promoting a balanced distribution (Singh & Joachims, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4f190",
   "metadata": {},
   "source": [
    "Rawlsian Maximin Exposure guarantees that even the least advantaged content or users receive exposure, preventing systemic bias in job recommendations, for example, on platforms like LinkedIn (Biega et al., 2018). However, prioritizing exposure fairness can sometimes reduce recommendation accuracy, as underrepresented content may detract from high-engagement items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546189c",
   "metadata": {},
   "source": [
    "Min-Max Ratio helps detect systemic bias in advertising and financial services, where high-paying jobs or financial opportunities disproportionately target specific demographics (Yang et al., 2021; Wang et al., 2023). Similarly, Disparate Impact Ratio detects indirect discrimination in loan recommendations or credit scoring, even when race or gender are not explicitly used as features (Zafar et al., 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a4649",
   "metadata": {},
   "source": [
    "User perceptions of fairness are also crucial. Proportion of Envy-Free Users measures how users perceive fairness in outcomes. In platforms like ride-sharing or food delivery, consistently fewer requests for certain drivers or restaurants, despite similar service quality, can create a sense of unfairness (Zafar et al., 2017). Optimizing this metric may, however, limit user-driven personalization, reducing engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8902b",
   "metadata": {},
   "source": [
    "Statistical Parity ensures recommendations are equally distributed across groups, particularly relevant in news recommendation systems where exposure bias can dominate certain viewpoints (Singh et al., 2018). Enforcing statistical parity may reduce personalization quality, as it prioritizes diversity over user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69132281",
   "metadata": {},
   "source": [
    "Demographic Parity ensures equal recommendation probabilities across demographic groups, helping prevent biased exposure in job recommendations (Feldman et al., 2015). Enforcing demographic parity may, however, hinder the system’s ability to optimize recommendations based on user behavior, potentially impacting user satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8df38",
   "metadata": {},
   "source": [
    "Metrics like the Gini Coefficient and Jain’s Index assess inequality in recommendation exposure, detecting whether a small group of top artists dominates streams, limiting diversity (Cini et al., 2020; Jain et al., 1984; Wang et al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cd724",
   "metadata": {},
   "source": [
    "These group fairness metrics highlight the challenge of balancing fairness, personalization, and efficiency in RSs. While fairness interventions improve equity, they introduce trade-offs that may affect recommendation quality and user engagement. Context-aware metric selection is essential to ensure that fairness interventions align with platform goals and stakeholder needs while addressing biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a746f70",
   "metadata": {},
   "source": [
    "Figure 4 Group fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c2283",
   "metadata": {},
   "source": [
    "## 4.3 Bridging Metrics: Balancing Personalization and Group Equity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643ddb3",
   "metadata": {},
   "source": [
    "Some metrics bridge individual and group fairness, offering balanced perspectives, especially in sensitive domains like healthcare or finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595e33a",
   "metadata": {},
   "source": [
    "Calibrated Fairness is vital in health recommender systems suggesting preventive care or health screenings. If the system predicts that a group needs care but systematically under-recommends it, calibration error is high, potentially worsening health disparities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f6dce",
   "metadata": {},
   "source": [
    "Similarly, Equalized Odds is crucial in credit card or loan recommendations, ensuring that error rates (false positives or false negatives) are consistent across protected groups. For example, if minority applicants are more likely to be falsely rejected, equalized odds metrics expose this unfairness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d4558",
   "metadata": {},
   "source": [
    "With Yelp for individual look at the ratings history and their patterns and look if they are getting the same recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162406b",
   "metadata": {},
   "source": [
    "Figure 5 Individual & group metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be8cd8",
   "metadata": {},
   "source": [
    "## 4.3.1. Conflicting Metrics and Practical Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542fd7d0",
   "metadata": {},
   "source": [
    "A major challenge in fairness evaluation is the trade-off between accuracy and fairness. While enforcing demographic parity can reduce biases in job recommendations (Feldman et al., 2015), it may lower the model’s ability to optimize for relevance, affecting user satisfaction. Similarly, exposure fairness may help underrepresented creators gain visibility but could lower engagement on content platforms (Singh & Joachims, 2018). Beyond trade-offs, practical challenges include measuring fairness at scale, defining fairness in diverse global markets, and addressing data sparsity issues in cold-start scenarios. Moreover, fairness interventions may require compliance with regulatory frameworks, such as the EU AI Act, which imposes constraints on algorithmic decision-making in high-risk domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93874715",
   "metadata": {},
   "source": [
    "## 4.4 Summary of Fairness Metrics in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6800b96",
   "metadata": {},
   "source": [
    "Fairness metrics in recommender systems serve distinct but interdependent roles. Individual fairness safeguards personalized experiences by preventing arbitrary treatment, while group fairness mitigates systemic biases, ensuring equitable exposure and opportunities across user and content groups. Bridging metrics like calibration and equalized odds help balance personalization and fairness, particularly in high-stakes domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0e964",
   "metadata": {},
   "source": [
    "These metrics do not operate in isolation. Enhancing exposure fairness for underrepresented groups may reduce individual recommendation accuracy, while prioritizing individual fairness could unintentionally reinforce existing group disparities. While fairness interventions aim to promote equity, different applications require different priorities. In loan approvals, fairness often focuses on avoiding disparate impact (Zafar et al., 2017), whereas in entertainment platforms, exposure fairness may be a greater concern (Cini et al., 2020). Evaluating fairness in practice requires domain-specific considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718035f6",
   "metadata": {},
   "source": [
    "Table 3 Formulas of the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72245ef",
   "metadata": {},
   "source": [
    "## 4.5 Formula explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358922b",
   "metadata": {},
   "source": [
    "Based on the metrics identified in the literature and considering the two datasets (MovieLens and Yelp), eight fairness metrics are selected for this study. Below, each metric is explained along with its mathematical formulation and the required components to measure fairness in RSs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5360c2d",
   "metadata": {},
   "source": [
    "Consistency Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8184ffe",
   "metadata": {},
   "source": [
    "Formula:\n",
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff804f1d",
   "metadata": {},
   "source": [
    ": Total number of users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bda37d",
   "metadata": {},
   "source": [
    ": Set of similar users (neighbours) of user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d5b05",
   "metadata": {},
   "source": [
    ": Distance between recommendation lists of user  and neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179073f",
   "metadata": {},
   "source": [
    "Counterfactual Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35e909",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2c2b0",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978162b",
   "metadata": {},
   "source": [
    ": Recommendation list for user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5320b5a",
   "metadata": {},
   "source": [
    ": Recommendation list after changing user ’s sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af477f57",
   "metadata": {},
   "source": [
    "E: Expectation over all users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246ec05",
   "metadata": {},
   "source": [
    "Local Individual Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c31b99",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3400278",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e7fa9",
   "metadata": {},
   "source": [
    ": Maximum difference within user ’s neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95635329",
   "metadata": {},
   "source": [
    ": Recommendation list for user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f017e5",
   "metadata": {},
   "source": [
    ": Recommendation list after changing user ’s sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64318d22",
   "metadata": {},
   "source": [
    ": Total number of users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f87d87",
   "metadata": {},
   "source": [
    "Calibration Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d1483",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adbade",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65868b",
   "metadata": {},
   "source": [
    ": Number of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebe99b",
   "metadata": {},
   "source": [
    ": Actual outcome for prediction iii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2494c",
   "metadata": {},
   "source": [
    ": Predicted probability for iii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a87e1",
   "metadata": {},
   "source": [
    "Disparate Impact Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca521494",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7238dab",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc492d67",
   "metadata": {},
   "source": [
    ": Positive recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a46198",
   "metadata": {},
   "source": [
    ": Protected group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37815f02",
   "metadata": {},
   "source": [
    ": Unprotected group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e80270",
   "metadata": {},
   "source": [
    "Statistical Parity of Exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b85d11",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee50ab",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f5e95",
   "metadata": {},
   "source": [
    "Measures the absolute difference in recommendation probabilities between protected and unprotected groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc5bc1",
   "metadata": {},
   "source": [
    "Rawlsian Maximin User Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603627c",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397495e2",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6bec4",
   "metadata": {},
   "source": [
    "​: Total exposure or recommendation utility for group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5338d",
   "metadata": {},
   "source": [
    "Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086bf80",
   "metadata": {},
   "source": [
    "Formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc36537",
   "metadata": {},
   "source": [
    "Where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd664f37",
   "metadata": {},
   "source": [
    "Ensures equal probability of recommendation between the protected and unprotected groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fcf1b",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62563ed6",
   "metadata": {},
   "source": [
    "Fairness in recommender systems remains a multifaceted challenge, requiring careful balancing between personalization, equity, and efficiency. As explored in this review, bias in RSs emerges from multiple sources, including algorithmic design, data limitations, and long-term feedback loops. These biases can reinforce societal inequalities, influencing access to job opportunities, financial services, and digital content visibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981e698",
   "metadata": {},
   "source": [
    "To address these concerns, fairness metrics play a critical role in evaluating and mitigating bias. Individual fairness metrics ensure that users with similar preferences receive comparable recommendations, reducing arbitrary discrepancies. Group fairness metrics help correct systemic imbalances, preventing historically disadvantaged groups from being overlooked. Bridging metrics, such as calibrated fairness and equalized odds, provide a middle ground between ensuring fair outcomes while maintaining personalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b94153",
   "metadata": {},
   "source": [
    "However, fairness in RSs is not a one-size-fits-all solution. Implementing fairness interventions requires navigating trade-offs—enhancing fairness may come at the cost of recommendation accuracy, engagement, or platform revenue. Furthermore, conflicting fairness objectives often arise, as fairness for users may not align with fairness for content providers or platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2b748",
   "metadata": {},
   "source": [
    "Given these complexities, future research and industry practices must focus on dynamic, context-aware approaches to fairness. This includes developing adaptive fairness mechanisms, refining evaluation frameworks, and ensuring compliance with ethical and regulatory guidelines. As RSs continue to shape online experiences, fairness must remain a core design principle to foster equitable and inclusive digital ecosystems."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
